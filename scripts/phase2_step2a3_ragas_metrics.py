"""
Phase 2 - Step 2a-3: Configure RAGAS Metrics
Set up and test RAGAS evaluation metrics for SOX compliance validation
"""

import json
import os
import pandas as pd
from typing import List, Dict, Any
from datetime import datetime
import sys

# Add backend path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'backend', 'app'))

# RAGAS imports
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy, 
    context_precision,
    context_recall
)
from datasets import Dataset
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def setup_ragas_components():
    """Initialize RAGAS LLM and embedding components"""
    print("🔧 Setting up RAGAS components...")
    
    # Initialize LLM for RAGAS evaluation
    evaluator_llm = LangchainLLMWrapper(ChatOpenAI(
        model="gpt-4",
        temperature=0.0  # Deterministic for evaluation
    ))
    
    # Initialize embeddings for RAGAS
    evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(
        model="text-embedding-3-small"
    ))
    
    print("✅ RAGAS components initialized")
    return evaluator_llm, evaluator_embeddings

def load_evaluation_dataset() -> pd.DataFrame:
    """Load the RAG pipeline evaluation dataset"""
    print("📋 Loading RAG pipeline evaluation dataset...")
    
    with open('data/evaluation_datasets/rag_pipeline_dataset.json', 'r') as f:
        dataset = json.load(f)
    
    print(f"✅ Loaded {len(dataset)} evaluation items")
    return dataset

def prepare_ragas_dataset(dataset: List[Dict[str, Any]]) -> Dataset:
    """Convert our dataset to RAGAS format"""
    print("🔄 Preparing dataset for RAGAS evaluation...")
    
    ragas_data = {
        "question": [],
        "answer": [],
        "contexts": [],
        "ground_truth": []
    }
    
    for item in dataset:
        # Question from our evaluation dataset
        ragas_data["question"].append(item["question"])
        
        # Answer will be generated by our RAG system (for now, use ground truth)
        # In a real evaluation, this would be the RAG system's response
        ragas_data["answer"].append(f"Based on the evidence provided, the compliance status is: {item['ground_truth']}")
        
        # Contexts from retrieved controls
        ragas_data["contexts"].append(item["contexts"])
        
        # Ground truth answer
        ragas_data["ground_truth"].append(item["ground_truth"])
    
    # Convert to HuggingFace Dataset
    ragas_dataset = Dataset.from_dict(ragas_data)
    
    print(f"✅ Prepared RAGAS dataset with {len(ragas_dataset)} items")
    return ragas_dataset

def configure_ragas_metrics(evaluator_llm, evaluator_embeddings):
    """Configure RAGAS metrics for SOX compliance evaluation"""
    print("⚙️ Configuring RAGAS metrics...")
    
    # Configure metrics for RAGAS 0.3.0
    metrics = [
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    ]
    
    print("✅ Configured RAGAS metrics:")
    print("  • Faithfulness: Measures if answer is grounded in context")
    print("  • Answer Relevancy: Measures if answer addresses the question")
    print("  • Context Precision: Measures if retrieved contexts are relevant")
    print("  • Context Recall: Measures if all relevant contexts were retrieved")
    
    return metrics

def run_ragas_evaluation(ragas_dataset: Dataset, metrics: List) -> Dict[str, Any]:
    """Run RAGAS evaluation on our dataset"""
    print("🚀 Running RAGAS evaluation...")
    print("⏳ This may take a few minutes as RAGAS evaluates each item...")
    
    try:
        # Run evaluation with RAGAS 0.3.0 API
        evaluation_result = evaluate(
            dataset=ragas_dataset,
            metrics=metrics,
            llm=ChatOpenAI(model="gpt-4", temperature=0.0),
            embeddings=OpenAIEmbeddings(model="text-embedding-3-small")
        )
        
        print("✅ RAGAS evaluation completed!")
        return evaluation_result
        
    except Exception as e:
        print(f"❌ Error during RAGAS evaluation: {e}")
        raise

def analyze_ragas_results(evaluation_result: Dict[str, Any], 
                         original_dataset: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Analyze and summarize RAGAS evaluation results"""
    print("📊 Analyzing RAGAS evaluation results...")
    
    # Extract metrics
    results_df = evaluation_result.to_pandas()
    
    # Calculate overall metrics
    overall_metrics = {}
    for metric in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']:
        if metric in results_df.columns:
            overall_metrics[metric] = {
                'mean': float(results_df[metric].mean()),
                'std': float(results_df[metric].std()),
                'min': float(results_df[metric].min()),
                'max': float(results_df[metric].max())
            }
    
    # Analyze by dataset source (original vs enhanced)
    source_analysis = {}
    for i, item in enumerate(original_dataset):
        source = item['evidence_metadata']['dataset_source']
        if source not in source_analysis:
            source_analysis[source] = {
                'count': 0,
                'metrics': {metric: [] for metric in overall_metrics.keys()}
            }
        
        source_analysis[source]['count'] += 1
        for metric in overall_metrics.keys():
            if metric in results_df.columns:
                source_analysis[source]['metrics'][metric].append(results_df.iloc[i][metric])
    
    # Calculate averages by source
    for source in source_analysis:
        for metric in source_analysis[source]['metrics']:
            values = source_analysis[source]['metrics'][metric]
            if values:
                source_analysis[source]['metrics'][metric] = {
                    'mean': sum(values) / len(values),
                    'count': len(values)
                }
    
    # Analyze by quality level
    quality_analysis = {}
    for i, item in enumerate(original_dataset):
        quality = item['evidence_metadata']['quality_level']
        if quality not in quality_analysis:
            quality_analysis[quality] = {
                'count': 0,
                'metrics': {metric: [] for metric in overall_metrics.keys()}
            }
        
        quality_analysis[quality]['count'] += 1
        for metric in overall_metrics.keys():
            if metric in results_df.columns:
                quality_analysis[quality]['metrics'][metric].append(results_df.iloc[i][metric])
    
    # Calculate averages by quality
    for quality in quality_analysis:
        for metric in quality_analysis[quality]['metrics']:
            values = quality_analysis[quality]['metrics'][metric]
            if values:
                quality_analysis[quality]['metrics'][metric] = {
                    'mean': sum(values) / len(values),
                    'count': len(values)
                }
    
    analysis = {
        'evaluation_timestamp': datetime.now().isoformat(),
        'total_items_evaluated': len(results_df),
        'overall_metrics': overall_metrics,
        'source_analysis': source_analysis,
        'quality_analysis': quality_analysis,
        'target_accuracy': 0.90,
        'performance_assessment': {}
    }
    
    # Assess performance vs targets
    for metric, values in overall_metrics.items():
        analysis['performance_assessment'][metric] = {
            'current_score': values['mean'],
            'target_met': values['mean'] >= 0.90,
            'gap_to_target': 0.90 - values['mean']
        }
    
    print("✅ RAGAS analysis completed")
    return analysis

def save_ragas_results(evaluation_result: Dict[str, Any], 
                      analysis: Dict[str, Any]) -> None:
    """Save RAGAS evaluation results"""
    print("💾 Saving RAGAS evaluation results...")
    
    # Create output directory
    os.makedirs('data/evaluation_results', exist_ok=True)
    
    # Save detailed results
    results_df = evaluation_result.to_pandas()
    results_df.to_json('data/evaluation_results/ragas_detailed_results.json', 
                      orient='records', indent=2)
    
    # Save analysis summary
    with open('data/evaluation_results/ragas_analysis_summary.json', 'w') as f:
        json.dump(analysis, f, indent=2)
    
    print("✅ Saved RAGAS results:")
    print("  📊 Detailed: data/evaluation_results/ragas_detailed_results.json")
    print("  📈 Summary: data/evaluation_results/ragas_analysis_summary.json")

def display_ragas_summary(analysis: Dict[str, Any]) -> None:
    """Display RAGAS evaluation summary"""
    print("\n" + "="*60)
    print("📊 RAGAS EVALUATION SUMMARY")
    print("="*60)
    
    print(f"📋 Total Items Evaluated: {analysis['total_items_evaluated']}")
    print(f"🎯 Target Accuracy: {analysis['target_accuracy']*100}%")
    
    print(f"\n📈 Overall Metrics:")
    for metric, values in analysis['overall_metrics'].items():
        score = values['mean']
        target_met = "✅" if score >= 0.90 else "❌"
        print(f"  {target_met} {metric.title()}: {score:.3f} (±{values['std']:.3f})")
    
    print(f"\n📊 Performance by Dataset Source:")
    for source, data in analysis['source_analysis'].items():
        print(f"  📁 {source.title()} ({data['count']} items):")
        for metric, values in data['metrics'].items():
            if isinstance(values, dict):
                print(f"    • {metric.title()}: {values['mean']:.3f}")
    
    print(f"\n📊 Performance by Quality Level:")
    for quality, data in analysis['quality_analysis'].items():
        print(f"  🏷️ {quality.title()} ({data['count']} items):")
        for metric, values in data['metrics'].items():
            if isinstance(values, dict):
                print(f"    • {metric.title()}: {values['mean']:.3f}")

def test_ragas_metrics():
    """Test RAGAS metrics configuration and evaluation"""
    print("🚀 Testing RAGAS Metrics Configuration")
    print("=" * 60)
    
    try:
        # Setup RAGAS components
        evaluator_llm, evaluator_embeddings = setup_ragas_components()
        
        # Load evaluation dataset
        dataset = load_evaluation_dataset()
        
        # Prepare RAGAS dataset
        ragas_dataset = prepare_ragas_dataset(dataset)
        
        # Configure metrics
        metrics = configure_ragas_metrics(evaluator_llm, evaluator_embeddings)
        
        # Run evaluation (this will take some time)
        evaluation_result = run_ragas_evaluation(ragas_dataset, metrics)
        
        # Analyze results
        analysis = analyze_ragas_results(evaluation_result, dataset)
        
        # Save results
        save_ragas_results(evaluation_result, analysis)
        
        # Display summary
        display_ragas_summary(analysis)
        
        print(f"\n✅ Step 2a-3 Complete: RAGAS Metrics Configured and Tested Successfully!")
        
        return evaluation_result, analysis
        
    except Exception as e:
        print(f"❌ Error in Step 2a-3: {e}")
        raise

if __name__ == "__main__":
    test_ragas_metrics() 