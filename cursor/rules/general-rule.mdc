---
description: 
globs: 
alwaysApply: false
---
# Veritarc AI - General Rules

## Development Workflow Rules
- You must always commit your changes whenever you update code
- You must always try and write code that is well documented (self-documenting or commented is fine)
- You must only work on a single feature at a time
- You must explain your decisions thoroughly to the user

## Project Overview
Veritarc AI is an AI-powered evidence validation system for audit, risk, and compliance. It uses advanced RAG (Retrieval-Augmented Generation) techniques and multi-agent systems to analyze control evidence documents and provide confidence-scored assessments.

## Tech Stack & Dependencies
- **Python**: 3.12+ (managed with UV)
- **LLM**: OpenAI GPT-4 with OpenAI embeddings (text-embedding-3-small)
- **AI Framework**: LangChain + LangGraph for orchestration and multi-agent workflows
- **Vector Database**: ChromaDB for similarity search and document storage
- **Backend**: FastAPI with Pydantic models
- **Frontend**: Streamlit (primary) with future React components
- **Evaluation**: RAGAS for performance assessment
- **Monitoring**: LangSmith for tracing and debugging
- **External APIs**: Tavily for real-time regulatory guidance
- **Document Processing**: PyPDF2, python-docx, Pillow, pytesseract for OCR

## Project Structure
```
├── backend/                    # FastAPI application
│   ├── main.py                # FastAPI entry point
│   ├── app/                   # Core business logic
│   ├── agents/                # LangGraph agents
│   ├── models/                # Pydantic schemas
│   └── services/              # External service integrations
├── frontend/                  # Streamlit interface
├── data/                      # Data storage and processing
│   ├── control_requirements/  # SOX, SOC2, ISO27001 frameworks
│   ├── synthetic_evidence/    # Generated test data
│   └── processed/             # Processed datasets
├── notebooks/                 # Jupyter notebooks for development
├── docs/                      # Project documentation
└── tests/                     # Test files
```

## Python Style & Coding Conventions
- Follow PEP 8 with Black formatting
- Use type hints consistently
- Prefer async/await for I/O operations
- Use Pydantic models for data validation
- Follow dependency injection patterns
- Always include proper error handling and logging
- Include type hints and docstrings
- Follow the existing project structure
- Consider performance and scalability
- Include relevant unit tests

## Environment & Dependencies
- Use UV for dependency management: `uv add package-name`
- Environment variables in `.env` file (never commit)
- Python virtual environment at `.venv/`
- All secrets should use environment variables

## Testing & Evaluation
- Include unit tests for new functionality
- Use RAGAS for RAG evaluation
- Include integration tests for API endpoints
- Consider edge cases in document processing

## Performance Targets
- Sub-30-second response times for evidence validation
- 90%+ accuracy on control assessments
- Support for files up to 50MB
- Scalable to 1000+ concurrent users

## Security Considerations
- Never log sensitive audit data
- Implement proper access controls
- Use secure file handling practices
- Encrypt sensitive data at rest and in transit
- Follow audit trail requirements
