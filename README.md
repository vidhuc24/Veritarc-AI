# ğŸ” Veritarc AI - Evidence Validator
## AI-Powered Verification for Audit, Risk & Compliance

Refer to all the rules files from cursor /rules folder

### ğŸ¯ Bootcamp Final Challenge Submission

This project will address the bootcamp challenge requirements through 7 comprehensive tasks, building an end-to-end AI application for audit evidence validation with intelligent document chat capabilities.

---

## ğŸ¬ Demo Video
ğŸ“¹ **[5-Minute Live Demo](https://www.loom.com/share/YOUR_LOOM_LINK_HERE)**
> *Live demonstration of the Veritarc AI application showing evidence upload, validation, document chat, and results*

---

## ğŸ“‹ Challenge Tasks Plan

### Task 1: Problem & Audience Definition âœ…
**Problem Statement:** *Audit teams spend 60-70% of their time manually reviewing control evidence documents, leading to inconsistent evaluations, delayed audit cycles, and increased compliance costs.*

**Target User:** Internal Audit Managers and Senior Auditors at mid-to-large enterprises

**Why This Matters:**
Audit professionals are drowning in evidence review work. A typical SOX audit requires reviewing thousands of documents - access reviews, change logs, financial reconciliations, system configurations. Each document must be evaluated against specific control requirements, but this process is entirely manual, subjective, and time-consuming. Senior auditors spend their expertise on repetitive document review instead of risk analysis and strategic recommendations.

This creates a cascade of problems: audit cycles extend beyond deadlines, junior staff make inconsistent evaluations, audit costs spiral upward, and organizations face regulatory scrutiny for incomplete or delayed compliance reporting.

### Task 2: Proposed Solution âœ…
**Solution Vision:**
Veritarc AI will transform evidence review from a manual bottleneck into an intelligent, consistent process. Auditors will upload evidence documents (PDFs, screenshots, logs) and specify the control framework and requirement. The system will instantly analyze the document against comprehensive control criteria, providing a confidence-scored assessment with specific recommendations. Additionally, users can engage in natural language conversations with documents to extract insights and understand compliance details.

**Technology Stack:**
- **LLM**: OpenAI GPT-4 - Superior reasoning for complex control requirement interpretation and document chat
- **Embedding Model**: OpenAI text-embedding-3-small - Proven performance for document similarity matching  
- **Orchestration**: LangChain + LangGraph - Enables multi-agent validation workflow and chat pipeline
- **Vector Database**: Qdrant - Fast similarity search for control requirements matching and document retrieval
- **Monitoring**: LangSmith - Real-time performance tracking and debugging
- **Evaluation**: RAGAS - Automated assessment of faithfulness, relevance, and context precision for chat responses
- **User Interface**: Streamlit - Rapid development for prototype demonstration with dual-mode interface
- **Serving**: FastAPI + Docker - Production-ready API with containerized deployment

**Agentic Reasoning:**
- Evidence Analysis Agent: Will extract key information from uploaded PDF documents
- Compliance Validation Agent: Will match evidence against SOX control requirements
- Risk Assessment Agent: Will evaluate completeness and identify potential gaps
- Recommendation Agent: Will suggest specific improvements or next steps
- Document Chat Agent: Will provide conversational interface for document exploration and Q&A

### Task 3: Data Strategy âœ…
**Data Sources:**
1. **SOX Control Requirements Database**: Public SOX framework documentation
2. **Synthetic Evidence Documents**: Will be generated using GPT-4 with known pass/fail labels
3. **Tavily Search API**: Real-time regulatory guidance and best practices
4. **Public SEC Filings**: Real-world SOX compliance documentation examples

**Chunking Strategy:** Semantic chunking with 500-token overlaps to preserve logical relationships in SOX control requirements while ensuring no critical connections are lost during retrieval.

### Task 4: End-to-End Prototype ğŸ”„
Will build complete agentic RAG application with:
- PDF file upload processing
- Multi-agent validation workflow using LangGraph
- Interactive document chat interface for Q&A
- Real-time evidence assessment with confidence scoring
- Dual-mode Streamlit interface (Validation + Chat)
- Local FastAPI deployment

### Task 5: Golden Test Dataset ğŸ”„
Will create comprehensive synthetic evaluation dataset with RAGAS framework:
- 100+ evidence scenarios covering high-quality, partial, poor, and edge cases
- Chat Q&A pairs for document interaction evaluation
- Baseline performance metrics to be established
- Automated evaluation pipeline for both validation and chat capabilities

### Task 6: Advanced Retrieval ğŸ”„
Will implement and test multiple advanced retrieval techniques:
- Hybrid Search: Semantic + keyword matching
- Query Expansion: Automatic compliance term expansion
- Multi-hop Retrieval: Cross-reference following
- Metadata Filtering: Framework-specific filtering
- Conversational Retrieval: Context-aware chat responses

### Task 7: Performance Assessment ğŸ”„
Will conduct comprehensive performance comparison across all retrieval methods with quantified improvements using RAGAS metrics for both validation accuracy and chat response quality.

---

## ğŸ¯ Key Features (Planned)

### Core Functionality
- **PDF Document Processing**: Streamlined PDF upload and text extraction
- **SOX Framework Support**: Focused on Sarbanes-Oxley control requirements
- **Confidence Scoring**: AI-generated confidence levels for each validation
- **Gap Analysis**: Specific recommendations for evidence improvement
- **Audit Trail**: Complete logging for regulatory compliance
- **Interactive Document Chat**: Natural language Q&A with uploaded documents

### Advanced AI Capabilities
- **Multi-Agent Validation**: Specialized agents for different validation aspects
- **Conversational Interface**: Chat with documents using natural language
- **Advanced Retrieval**: Hybrid search, query expansion, multi-hop retrieval
- **Continuous Learning**: Model improvement from user feedback
- **Real-time Processing**: Sub-30-second response times
- **Context-Aware Responses**: Chat maintains conversation history and document context

### User Experience
- **Intuitive Interface**: Simple drag-and-drop PDF upload
- **Dual-Mode Interface**: Switch between Validation and Chat modes
- **Instant Results**: Real-time validation with detailed explanations
- **Interactive Q&A**: Ask questions about document content and compliance
- **Export Capabilities**: PDF reports for audit documentation
- **Responsive Design**: Works on desktop and mobile devices

---

## ğŸ“Š Performance Targets

### RAGAS Evaluation Goals
| Metric | Validation Baseline | Chat Baseline | Target with Hybrid Search | Target with Query Expansion | Target with Multi-hop |
|--------|-------------|-------------|----------------|------------------|-------------|
| Faithfulness | 0.85 | 0.80 | 0.88 | 0.89 | 0.91 |
| Answer Relevancy | 0.80 | 0.75 | 0.83 | 0.85 | 0.87 |
| Context Precision | 0.75 | 0.70 | 0.82 | 0.84 | 0.86 |
| Context Recall | 0.70 | 0.65 | 0.76 | 0.78 | 0.82 |

### Business Impact Goals
- **Time Savings**: 70% reduction in evidence review time
- **Consistency**: 95% agreement with expert assessments
- **User Engagement**: 80% of users utilize chat feature for document exploration
- **Cost Reduction**: $200K+ annual savings for mid-size audit teams

---

## ğŸ”® Future Roadmap

### Phase 2 Enhancements
- **Multi-format Support**: DOCX, images with OCR, CSV files
- **Additional Frameworks**: SOC2, ISO27001 compliance support
- **Fine-tuned Models**: Custom embedding models for audit terminology
- **Multi-modal Processing**: Advanced chart and table analysis
- **Integration APIs**: Connect with major GRC platforms
- **Mobile Application**: Native iOS/Android apps
- **Advanced Chat Features**: Multi-document conversations, chat history, bookmarking

### Scaling Opportunities
- **Enterprise Deployment**: Multi-tenant SaaS platform
- **Industry Specialization**: Healthcare, financial services, manufacturing
- **International Expansion**: Support for global compliance frameworks
- **AI Consulting**: Professional services for custom implementations

---

## ğŸ† Bootcamp Learning Integration

This project will demonstrate mastery of key bootcamp concepts:

- **Session 2**: RAG implementation with embeddings and vector databases
- **Session 4**: Production-grade LangChain/LCEL pipelines for validation and chat
- **Session 5-6**: Multi-agent systems with LangGraph
- **Session 7**: Synthetic data generation and LangSmith monitoring
- **Session 8**: Comprehensive evaluation with RAGAS for both validation and chat responses
- **Session 9**: Advanced retrieval techniques and optimization

---

## ğŸ‘¨â€ğŸ’» Developer

**Vidhu C** - AI Engineering Bootcamp Participant
- GitHub: [@vidhuc24](https://github.com/vidhuc24)
- Project Repository: [AIE_Bootcamp_VC](https://github.com/vidhuc24/AIE_Bootcamp_VC)

---

## ğŸ“„ License

This project is part of the AI Makerspace Engineering Bootcamp certification challenge.

---

---

## ğŸš€ Quick Start

### Prerequisites
```bash
# Ensure you have UV installed (recommended) or Python 3.12+
uv --version  # Should show UV version
# OR
python3 --version  # Should show Python 3.12+
```

### Installation

#### Option 1: Using UV (Recommended)
```bash
# Initialize UV project and install dependencies
uv init --no-readme
uv add --requirement requirements.txt

# Set up environment variables
cp .env.example .env
# Add your OpenAI API key to .env
```

#### Option 2: Using Traditional Virtual Environment
```bash
# Create and activate virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Add your OpenAI API key to .env
```

### ğŸ” How to Verify Your Environment is Active

After installation, verify your setup is working correctly:

#### Quick Verification Commands
```bash
# Check Python location (should show your project's .venv path)
which python

# Check virtual environment variable
echo $VIRTUAL_ENV

# Test key package imports
python -c "import openai, langchain, streamlit, fastapi; print('âœ… All packages working!')"
```

#### Expected Outputs
```bash
# Virtual environment should be active:
which python
# Expected: /path/to/your/project/.venv/bin/python

echo $VIRTUAL_ENV  
# Expected: /path/to/your/project/.venv

# Packages should import successfully:
python -c "import openai, streamlit; print('âœ… Working!')"
# Expected: âœ… Working!
```

#### Troubleshooting
- **Empty parentheses `()` in prompt**: Normal with UV environments - your environment is still active
- **`pip` not found**: With UV, use `uv add package-name` instead of `pip install`
- **Package import errors**: Ensure you're in the activated environment

### Run the Application

#### Using UV (Recommended)
```bash
# Start the backend
uv run python backend/main.py

# In another terminal, start the frontend
uv run streamlit run frontend/streamlit_app.py
```

#### Using Traditional Virtual Environment
```bash
# Ensure environment is activated first
source .venv/bin/activate

# Start the backend
cd backend && python main.py

# In another terminal, start the frontend
cd frontend && streamlit run streamlit_app.py
```

### Access the Application
- **Frontend**: http://localhost:8501 (Validation & Chat Interface)
- **API Documentation**: http://localhost:8000/docs

---

## ğŸ“ Project Structure

```
auditflow-ai/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ .env.example                      # Environment variables template
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                       # FastAPI application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ evidence_processor.py     # Document processing
â”‚   â”‚   â”œâ”€â”€ validation_engine.py      # RAG + LLM validation
â”‚   â”‚   â”œâ”€â”€ retrieval_engine.py       # Advanced retrieval methods
â”‚   â”‚   â”œâ”€â”€ chat_engine.py            # Document chat interface
â”‚   â”‚   â””â”€â”€ vector_store.py           # Qdrant vector database management
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ evidence_agent.py         # Evidence analysis agent
â”‚   â”‚   â”œâ”€â”€ compliance_agent.py       # Compliance validation agent
â”‚   â”‚   â”œâ”€â”€ recommendation_agent.py   # Recommendation agent
â”‚   â”‚   â””â”€â”€ chat_agent.py             # Document chat agent
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ schemas.py                # Pydantic models
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ streamlit_app.py              # Main Streamlit interface (Validation + Chat)
â”‚   â””â”€â”€ src/                          # React components (future)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ control_requirements/         # SOX framework documentation
â”‚   â”œâ”€â”€ synthetic_evidence/           # Generated test evidence
â”‚   â”œâ”€â”€ enhanced_evidence/            # RAGAS-enhanced evidence
â”‚   â”œâ”€â”€ evaluation_datasets/          # RAGAS evaluation datasets
â”‚   â””â”€â”€ processed/                    # Processed datasets
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_synthetic_evidence.py # Synthetic data creation
â”‚   â”œâ”€â”€ ragas_*.py                    # RAGAS integration scripts
â”‚   â””â”€â”€ evaluation_*.py               # Evaluation pipelines
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_generation.ipynb      # Synthetic data creation
â”‚   â”œâ”€â”€ 02_rag_pipeline.ipynb         # RAG development
â”‚   â”œâ”€â”€ 03_evaluation.ipynb           # RAGAS evaluation
â”‚   â”œâ”€â”€ 04_advanced_retrieval.ipynb   # Advanced techniques testing
â”‚   â””â”€â”€ 05_chat_interface.ipynb       # Document chat development
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_validation_engine.py     # Unit tests
â”‚   â”œâ”€â”€ test_chat_engine.py           # Chat functionality tests
â”‚   â””â”€â”€ test_agents.py                # Agent tests
â””â”€â”€ docs/
    â”œâ”€â”€ CHALLENGE_RESPONSES.md         # Detailed task responses
    â”œâ”€â”€ TECHNICAL_ARCHITECTURE.md     # System design
    â””â”€â”€ EVALUATION_RESULTS.md         # Performance analysis
```

---